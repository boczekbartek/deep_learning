{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vugrad as vg\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vg.TensorNode(np.random.randn(2, 2))\n",
    "a = vg.TensorNode(np.random.randn(2, 2))\n",
    "b = vg.TensorNode(np.random.randn(2, 2))\n",
    "c=a+b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does c.value contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.37443041,  2.90527094],\n",
       "       [ 0.59149453,  0.39987651]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<vugrad.core.OpNode object at 0x7fb1aca96610>\n",
      "<class 'vugrad.core.Add'>\n"
     ]
    }
   ],
   "source": [
    "print(c.source)\n",
    "print(c.source.op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<vugrad.core.TensorNode at 0x7fb1acaa1be0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.source.inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "1) The operation of OpNNode object is defined by the Op abstract class, which can be subclassed to define certain operations. Those operations have to implement *forward* and *backward* methods, as defined in Op interface.\n",
    "\n",
    "2) It is defined in core.py:287\n",
    "```python\n",
    "    class Add(Op):\n",
    "        \"\"\"\n",
    "        Op for element-wise matrix addition.\n",
    "        \"\"\"\n",
    "        @staticmethod\n",
    "        def forward(context, a, b):\n",
    "            assert a.shape == b.shape, f'Arrays not the same sizes ({a.shape} {b.shape}).'\n",
    "            return a + b\n",
    "\n",
    "```\n",
    "3) It's done because the computational graph is eagerly executed, it's build on the fly. The graph only defines the flow of the computations, but not their results. OpNode is connected to the output nodes in core.py:212 \n",
    "\n",
    "```python\n",
    "    outputs = [TensorNode(value=output, source=opnode) for output in outputs_raw]\n",
    "    opnode.outputs = outputs\n",
    "```\n",
    "\n",
    "# Question 7\n",
    "```python\n",
    "    # compute the gradients over the inputs\n",
    "    ginputs_raw = self.op.backward(self.context, *goutputs_raw)\n",
    "```\n",
    "\n",
    "# Question 8\n",
    "\n",
    "TODO\n",
    "\n",
    "# Question 9\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(vg.ops.Op):\n",
    "    \"\"\"\n",
    "    Op for element-wise application of ReLU function\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(context, input):\n",
    "        #print(input.shape)\n",
    "        relux = input * (input > 0)\n",
    "        context['relux'] = relux\n",
    "        return relux\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(context, goutput):\n",
    "        relux = context['relux']\n",
    "        drelux = np.greater(relux, 0).astype(int)\n",
    "        return drelux*goutput\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\" Wrap the sigmoid op in a funciton (just for symmetry with the softmax). \"\"\"\n",
    "    return ReLU.do_forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## loaded data:\n",
      "         number of instances: 55000 in training, 5000 in validation\n",
      " training class distribution: [5434 6212 5465 5622 5343 4963 5436 5702 5357 5466]\n",
      "     val. class distribution: [489 530 493 509 499 458 482 563 494 483]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import vugrad as vg\n",
    "\n",
    "# Parse command line arguments\n",
    "\n",
    "class args:\n",
    "    lr = 0.0001\n",
    "    data = 'mnist'\n",
    "    epochs = 20\n",
    "    batch_size = 128\n",
    "\n",
    "# Create a simple neural network.\n",
    "# This is a `Module` consisting of other modules representing linear layers, provided by the vugrad library.\n",
    "class MLP(vg.Module):\n",
    "    \"\"\"\n",
    "    A simple MLP with one hidden layer, and a sigmoid non-linearity on the hidden layer and a softmax on the\n",
    "    output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_mult=4):\n",
    "        \"\"\"\n",
    "        :param input_size:\n",
    "        :param output_size:\n",
    "        :param hidden_mult: Multiplier that indicates how many times bigger the hidden layer is than the input layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_size = hidden_mult * input_size\n",
    "        # -- There is no common wisdom on how big the hidden size should be, apart from the idea\n",
    "        #    that it should be strictly _bigger_ than the input if at all possible.\n",
    "\n",
    "        self.layer1 = vg.Linear(input_size, hidden_size)\n",
    "        self.layer2 = vg.Linear(hidden_size, output_size)\n",
    "        # -- The linear layer (without activation) is implemented in vugrad. We simply instantiate these modules, and\n",
    "        #    add them to our network.\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        assert len(input.size()) == 2\n",
    "\n",
    "        # first layer\n",
    "        hidden = self.layer1(input)\n",
    "\n",
    "        # non-linearity\n",
    "        hidden = vg.sigmoid(hidden)\n",
    "        # -- We've called a utility function here, to mimin how this is usually done in pytorch. We could also do:\n",
    "        #    hidden = Sigmoid.do_forward(hidden)\n",
    "\n",
    "        # second layer\n",
    "        output = self.layer2(hidden)\n",
    "\n",
    "        # softmax activation\n",
    "        output = vg.logsoftmax(output)\n",
    "        # -- the logsoftmax computes the _logarithm_ of the probabilities produced by softmax. This makes the computation\n",
    "        #    of the CE loss more stable when the probabilities get close to 0 (remember that the CE loss is the logarithm\n",
    "        #    of these probabilities). It needs to be implemented in a specific way. See the source for details.\n",
    "\n",
    "        return output\n",
    "\n",
    "    def parameters(self):\n",
    "\n",
    "        return self.layer1.parameters() + self.layer2.parameters()\n",
    "\n",
    "\n",
    "class MLP_ReLU(MLP):\n",
    "    def forward(self, input):\n",
    "\n",
    "        assert len(input.size()) == 2\n",
    "\n",
    "        # first layer\n",
    "        hidden = self.layer1(input)\n",
    "\n",
    "        # non-linearity\n",
    "        hidden = relu(hidden)\n",
    "        # -- We've called a utility function here, to mimin how this is usually done in pytorch. We could also do:\n",
    "        #    hidden = Sigmoid.do_forward(hidden)\n",
    "\n",
    "        # second layer\n",
    "        output = self.layer2(hidden)\n",
    "\n",
    "        # softmax activation\n",
    "        output = vg.logsoftmax(output)\n",
    "        # -- the logsoftmax computes the _logarithm_ of the probabilities produced by softmax. This makes the computation\n",
    "        #    of the CE loss more stable when the probabilities get close to 0 (remember that the CE loss is the logarithm\n",
    "        #    of these probabilities). It needs to be implemented in a specific way. See the source for details.\n",
    "\n",
    "        return output\n",
    "\n",
    "## Load the data\n",
    "if args.data == 'synth':\n",
    "    (xtrain, ytrain), (xval, yval), num_classes = vg.load_synth()\n",
    "elif args.data == 'mnist':\n",
    "    (xtrain, ytrain), (xval, yval), num_classes = vg.load_mnist(final=False, flatten=True)\n",
    "else:\n",
    "    raise Exception(f'Dataset {args.data} not recognized.')\n",
    "\n",
    "print(f'## loaded data:')\n",
    "print(f'         number of instances: {xtrain.shape[0]} in training, {xval.shape[0]} in validation')\n",
    "print(f' training class distribution: {np.bincount(ytrain)}')\n",
    "print(f'     val. class distribution: {np.bincount(yval)}')\n",
    "\n",
    "num_instances, num_features = xtrain.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Starting training\n",
      "epoch 000\n",
      "       accuracy: 0.0998\n",
      "   running loss: 0.4009\n",
      "epoch 001\n",
      "       accuracy: 0.9496\n",
      "   running loss: 0.2092\n",
      "epoch 002\n",
      "       accuracy: 0.9596\n",
      "   running loss: 0.1586\n",
      "epoch 003\n",
      "       accuracy: 0.9634\n",
      "   running loss: 0.1262\n",
      "epoch 004\n",
      "       accuracy: 0.966\n",
      "   running loss: 0.1028\n",
      "epoch 005\n",
      "       accuracy: 0.9654\n",
      "   running loss: 0.08435\n",
      "epoch 006\n",
      "       accuracy: 0.969\n",
      "   running loss: 0.07036\n",
      "epoch 007\n",
      "       accuracy: 0.9692\n",
      "   running loss: 0.0593\n",
      "epoch 008\n",
      "       accuracy: 0.9702\n",
      "   running loss: 0.05076\n",
      "epoch 009\n",
      "       accuracy: 0.9696\n",
      "   running loss: 0.04395\n",
      "epoch 010\n",
      "       accuracy: 0.9706\n",
      "   running loss: 0.03853\n",
      "epoch 011\n",
      "       accuracy: 0.971\n",
      "   running loss: 0.03422\n",
      "epoch 012\n",
      "       accuracy: 0.9722\n",
      "   running loss: 0.03068\n",
      "epoch 013\n",
      "       accuracy: 0.9722\n",
      "   running loss: 0.02774\n",
      "epoch 014\n",
      "       accuracy: 0.9722\n",
      "   running loss: 0.0253\n",
      "epoch 015\n",
      "       accuracy: 0.972\n",
      "   running loss: 0.0233\n",
      "epoch 016\n",
      "       accuracy: 0.9716\n",
      "   running loss: 0.02146\n",
      "epoch 017\n",
      "       accuracy: 0.9722\n",
      "   running loss: 0.01991\n",
      "epoch 018\n",
      "       accuracy: 0.9726\n",
      "   running loss: 0.01855\n",
      "epoch 019\n",
      "       accuracy: 0.9718\n",
      "   running loss: 0.01734\n",
      "\n",
      "## Starting training\n",
      "epoch 000\n",
      "       accuracy: 0.1404\n",
      "   running loss: 2.296e+03\n",
      "epoch 001\n",
      "       accuracy: 0.8182\n",
      "   running loss: 0.7239\n",
      "epoch 002\n",
      "       accuracy: 0.8936\n",
      "   running loss: 0.5314\n",
      "epoch 003\n",
      "       accuracy: 0.7508\n",
      "   running loss: 0.8149\n",
      "epoch 004\n",
      "       accuracy: 0.8672\n",
      "   running loss: 0.4374\n",
      "epoch 005\n",
      "       accuracy: 0.9252\n",
      "   running loss: 0.3707\n",
      "epoch 006\n",
      "       accuracy: 0.9062\n",
      "   running loss: 0.3749\n",
      "epoch 007\n",
      "       accuracy: 0.9312\n",
      "   running loss: 0.2499\n",
      "epoch 008\n",
      "       accuracy: 0.946\n",
      "   running loss: 0.2132\n",
      "epoch 009\n",
      "       accuracy: 0.954\n",
      "   running loss: 0.2135\n",
      "epoch 010\n",
      "       accuracy: 0.9532\n",
      "   running loss: 0.1787\n",
      "epoch 011\n",
      "       accuracy: 0.9388\n",
      "   running loss: 0.1637\n",
      "epoch 012\n",
      "       accuracy: 0.9572\n",
      "   running loss: 0.139\n",
      "epoch 013\n",
      "       accuracy: 0.9604\n",
      "   running loss: 0.1259\n",
      "epoch 014\n",
      "       accuracy: 0.9624\n",
      "   running loss: 0.1054\n",
      "epoch 015\n",
      "       accuracy: 0.9612\n",
      "   running loss: 0.09876\n",
      "epoch 016\n",
      "       accuracy: 0.9622\n",
      "   running loss: 0.09676\n",
      "epoch 017\n",
      "       accuracy: 0.9624\n",
      "   running loss: 0.09136\n",
      "epoch 018\n",
      "       accuracy: 0.9636\n",
      "   running loss: 0.08402\n",
      "epoch 019\n",
      "       accuracy: 0.9624\n",
      "   running loss: 0.0806\n"
     ]
    }
   ],
   "source": [
    "results = list()\n",
    "## Create the model.\n",
    "\n",
    "n, m = xtrain.shape\n",
    "b = args.batch_size\n",
    "\n",
    "for nonlinearity, model in [('sigmoid', MLP), ('relu', MLP_ReLU)]:\n",
    "    ## Instantiate the model\n",
    "    mlp = model(input_size=num_features, output_size=num_classes)\n",
    "\n",
    "    print('\\n## Starting training')\n",
    "    for epoch in range(args.epochs):\n",
    "\n",
    "        print(f'epoch {epoch:03}')\n",
    "\n",
    "        ## Compute validation accuracy\n",
    "        o = mlp(vg.TensorNode(xval))\n",
    "        oval = o.value\n",
    "\n",
    "        predictions = np.argmax(oval, axis=1)\n",
    "        num_correct = (predictions == yval).sum()\n",
    "        acc = num_correct / yval.shape[0]\n",
    "\n",
    "        o.clear() # gc the computation graph\n",
    "        print(f'       accuracy: {acc:.4}')\n",
    "\n",
    "        cl = 0.0 # running sum of the training loss\n",
    "\n",
    "        # We loop over the data in batches of size `b`\n",
    "        for fr in range(0, n, b):\n",
    "\n",
    "            # The end index of the batch\n",
    "            to = min(fr + b, n)\n",
    "\n",
    "            # Slice out the batch and its corresponding target values\n",
    "            batch, targets = xtrain[fr:to, :], ytrain[fr:to]\n",
    "\n",
    "            # Wrap the inputs in a Node\n",
    "            batch = vg.TensorNode(value=batch)\n",
    "\n",
    "            outputs = mlp(batch)\n",
    "            loss = vg.logceloss(outputs, targets)\n",
    "            # -- The computation graph is now complete. It consists of the MLP, together with the computation of\n",
    "            #    the scalar loss.\n",
    "            # -- The variable `loss` is the TensorNode at the very top of our computation graph. This means we can call\n",
    "            #    it to perform operations on the computation graph, like clearing the gradients, starting the backpropgation\n",
    "            #    and clearing the graph.\n",
    "            # -- Note that we set the MLP up to produce log probabilties, so we should compute the CE loss for these.\n",
    "\n",
    "            cl += loss.value\n",
    "            # -- We must be careful here to extract the _raw_ value for the running loss. What would happen if we kept\n",
    "            #    a running sum using the TensorNode?\n",
    "\n",
    "            # Start the backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # pply gradient descent\n",
    "            for parm in mlp.parameters():\n",
    "                parm.value -= args.lr * parm.grad\n",
    "                # -- Note that we are directly manipulating the members of the parm TensorNode. This means that for this\n",
    "                #    part, we are not building up a computation graph.\n",
    "\n",
    "            # -- In Pytorch, the gradient descent is abstracted away into an Optimizer. This allows us to build slightly more\n",
    "            #    complexoptimizers than plain graident descent.\n",
    "\n",
    "            # Finally, we need to reset the gradients to zero ...\n",
    "            loss.zero_grad()\n",
    "            # ... and delete the parts of the computation graph we don't need to remember.\n",
    "            loss.clear()\n",
    "        results.append({\n",
    "                        'epoch' : epoch,\n",
    "                        'accuracy' : acc,\n",
    "                        'loss' : cl/n,\n",
    "                        'method' : nonlinearity,\n",
    "                        'dataset' : args.data\n",
    "                    })\n",
    "        print(f'   running loss: {cl/n:.4}')\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "df = pd.DataFrame(results)\n",
    "sns.lineplot(data=df, x='epoch', y='accuracy', hue='method')\n",
    "plt.savefig('simrelu.png')\n",
    "\n",
    "for f in ('sigmoid', 'relu'):\n",
    "    print(f, df[df.method == f].max()['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Starting training\n",
      "epoch 000\n",
      "       accuracy: 0.1098\n",
      "   running loss: 0.3616\n",
      "epoch 001\n",
      "       accuracy: 0.9534\n",
      "   running loss: 0.1637\n",
      "epoch 002\n",
      "       accuracy: 0.9636\n",
      "   running loss: 0.111\n",
      "epoch 003\n",
      "       accuracy: 0.9654\n",
      "   running loss: 0.07933\n",
      "epoch 004\n",
      "       accuracy: 0.9678\n",
      "   running loss: 0.05886\n",
      "epoch 005\n",
      "       accuracy: 0.969\n",
      "   running loss: 0.04537\n",
      "epoch 006\n",
      "       accuracy: 0.9696\n",
      "   running loss: 0.03613\n",
      "epoch 007\n",
      "       accuracy: 0.9706\n",
      "   running loss: 0.02965\n",
      "epoch 008\n",
      "       accuracy: 0.9716\n",
      "   running loss: 0.02496\n",
      "epoch 009\n",
      "       accuracy: 0.972\n",
      "   running loss: 0.02144\n",
      "epoch 010\n",
      "       accuracy: 0.9718\n",
      "   running loss: 0.01872\n",
      "epoch 011\n",
      "       accuracy: 0.9724\n",
      "   running loss: 0.01656\n",
      "epoch 012\n",
      "       accuracy: 0.9728\n",
      "   running loss: 0.01481\n",
      "epoch 013\n",
      "       accuracy: 0.9732\n",
      "   running loss: 0.01337\n",
      "epoch 014\n",
      "       accuracy: 0.9732\n",
      "   running loss: 0.01216\n",
      "epoch 015\n",
      "       accuracy: 0.9734\n",
      "   running loss: 0.01114\n",
      "epoch 016\n",
      "       accuracy: 0.974\n",
      "   running loss: 0.01026\n",
      "epoch 017\n",
      "       accuracy: 0.9742\n",
      "   running loss: 0.009504\n",
      "epoch 018\n",
      "       accuracy: 0.9742\n",
      "   running loss: 0.008848\n",
      "epoch 019\n",
      "       accuracy: 0.9744\n",
      "   running loss: 0.008272\n",
      "\n",
      "## Starting training\n",
      "epoch 000\n",
      "       accuracy: 0.0826\n",
      "   running loss: 2.722e+03\n",
      "epoch 001\n",
      "       accuracy: 0.7092\n",
      "   running loss: 0.8571\n",
      "epoch 002\n",
      "       accuracy: 0.9196\n",
      "   running loss: 0.5285\n",
      "epoch 003\n",
      "       accuracy: 0.8888\n",
      "   running loss: 0.5694\n",
      "epoch 004\n",
      "       accuracy: 0.8948\n",
      "   running loss: 0.4133\n",
      "epoch 005\n",
      "       accuracy: 0.9466\n",
      "   running loss: 0.2761\n",
      "epoch 006\n",
      "       accuracy: 0.9524\n",
      "   running loss: 0.222\n",
      "epoch 007\n",
      "       accuracy: 0.959\n",
      "   running loss: 0.1651\n",
      "epoch 008\n",
      "       accuracy: 0.9586\n",
      "   running loss: 0.1473\n",
      "epoch 009\n",
      "       accuracy: 0.9596\n",
      "   running loss: 0.1269\n",
      "epoch 010\n",
      "       accuracy: 0.9608\n",
      "   running loss: 0.1117\n",
      "epoch 011\n",
      "       accuracy: 0.9626\n",
      "   running loss: 0.0996\n",
      "epoch 012\n",
      "       accuracy: 0.9548\n",
      "   running loss: 0.09159\n",
      "epoch 013\n",
      "       accuracy: 0.966\n",
      "   running loss: 0.0812\n",
      "epoch 014\n",
      "       accuracy: 0.9642\n",
      "   running loss: 0.082\n",
      "epoch 015\n",
      "       accuracy: 0.9644\n",
      "   running loss: 0.08842\n",
      "epoch 016\n",
      "       accuracy: 0.966\n",
      "   running loss: 0.07331\n",
      "epoch 017\n",
      "       accuracy: 0.9636\n",
      "   running loss: 0.06665\n",
      "epoch 018\n",
      "       accuracy: 0.9628\n",
      "   running loss: 0.08\n",
      "epoch 019\n",
      "       accuracy: 0.9646\n",
      "   running loss: 0.08549\n"
     ]
    }
   ],
   "source": [
    "results = list()\n",
    "## Create the model.\n",
    "\n",
    "n, m = xtrain.shape\n",
    "b = args.batch_size\n",
    "\n",
    "for nonlinearity, model in [('sigmoid', MLP), ('relu', MLP_ReLU)]:\n",
    "    ## Instantiate the model\n",
    "    mlp = model(input_size=num_features, output_size=num_classes, hidden_mult=8)\n",
    "\n",
    "    print('\\n## Starting training')\n",
    "    for epoch in range(args.epochs):\n",
    "\n",
    "        print(f'epoch {epoch:03}')\n",
    "\n",
    "        ## Compute validation accuracy\n",
    "        o = mlp(vg.TensorNode(xval))\n",
    "        oval = o.value\n",
    "\n",
    "        predictions = np.argmax(oval, axis=1)\n",
    "        num_correct = (predictions == yval).sum()\n",
    "        acc = num_correct / yval.shape[0]\n",
    "\n",
    "        o.clear() # gc the computation graph\n",
    "        print(f'       accuracy: {acc:.4}')\n",
    "\n",
    "        cl = 0.0 # running sum of the training loss\n",
    "\n",
    "        # We loop over the data in batches of size `b`\n",
    "        for fr in range(0, n, b):\n",
    "\n",
    "            # The end index of the batch\n",
    "            to = min(fr + b, n)\n",
    "\n",
    "            # Slice out the batch and its corresponding target values\n",
    "            batch, targets = xtrain[fr:to, :], ytrain[fr:to]\n",
    "\n",
    "            # Wrap the inputs in a Node\n",
    "            batch = vg.TensorNode(value=batch)\n",
    "\n",
    "            outputs = mlp(batch)\n",
    "            loss = vg.logceloss(outputs, targets)\n",
    "            # -- The computation graph is now complete. It consists of the MLP, together with the computation of\n",
    "            #    the scalar loss.\n",
    "            # -- The variable `loss` is the TensorNode at the very top of our computation graph. This means we can call\n",
    "            #    it to perform operations on the computation graph, like clearing the gradients, starting the backpropgation\n",
    "            #    and clearing the graph.\n",
    "            # -- Note that we set the MLP up to produce log probabilties, so we should compute the CE loss for these.\n",
    "\n",
    "            cl += loss.value\n",
    "            # -- We must be careful here to extract the _raw_ value for the running loss. What would happen if we kept\n",
    "            #    a running sum using the TensorNode?\n",
    "\n",
    "            # Start the backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # pply gradient descent\n",
    "            for parm in mlp.parameters():\n",
    "                parm.value -= args.lr * parm.grad\n",
    "                # -- Note that we are directly manipulating the members of the parm TensorNode. This means that for this\n",
    "                #    part, we are not building up a computation graph.\n",
    "\n",
    "            # -- In Pytorch, the gradient descent is abstracted away into an Optimizer. This allows us to build slightly more\n",
    "            #    complexoptimizers than plain graident descent.\n",
    "\n",
    "            # Finally, we need to reset the gradients to zero ...\n",
    "            loss.zero_grad()\n",
    "            # ... and delete the parts of the computation graph we don't need to remember.\n",
    "            loss.clear()\n",
    "        results.append({\n",
    "                        'epoch' : epoch,\n",
    "                        'accuracy' : acc,\n",
    "                        'loss' : cl/n,\n",
    "                        'method' : nonlinearity,\n",
    "                        'dataset' : args.data\n",
    "                    })\n",
    "        print(f'   running loss: {cl/n:.4}')\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid 0.9744\n",
      "relu 0.966\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "df = pd.DataFrame(results)\n",
    "sns.lineplot(data=df, x='epoch', y='accuracy', hue='method')\n",
    "plt.savefig('simrelu_8xhidden.png')\n",
    "\n",
    "for f in ('sigmoid', 'relu'):\n",
    "    print(f, df[df.method == f].max()['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4ee4b3bd58dba8b15a3a54ac18766444a5e3c5a9d8db984134e1e61525b723b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('aml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
