{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vugrad as vg\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "1) The operation of OpNNode object is defined by the Op abstract class, which can be subclassed to define certain operations. Those operations have to implement *forward* and *backward* methods, as defined in Op interface.\n",
    "\n",
    "2) It is defined in core.py:287\n",
    "```python\n",
    "    class Add(Op):\n",
    "        \"\"\"\n",
    "        Op for element-wise matrix addition.\n",
    "        \"\"\"\n",
    "        @staticmethod\n",
    "        def forward(context, a, b):\n",
    "            assert a.shape == b.shape, f'Arrays not the same sizes ({a.shape} {b.shape}).'\n",
    "287         return a + b\n",
    "\n",
    "```\n",
    "3) It's done because the computational graph is eagerly executed, it's build on the fly. The graph only defines the flow of the computations, but not their results. OpNode is connected to the output nodes in core.py:212 \n",
    "\n",
    "```python\n",
    "    outputs = [TensorNode(value=output, source=opnode) for output in outputs_raw]\n",
    "212 opnode.outputs = outputs\n",
    "```\n",
    "\n",
    "# Question 7\n",
    "```python\n",
    "    # compute the gradients over the inputs\n",
    "132 ginputs_raw = self.op.backward(self.context, *goutputs_raw)\n",
    "```\n",
    "\n",
    "# Question 8\n",
    "\n",
    "TODO\n",
    "\n",
    "# Question 9\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(vg.ops.Op):\n",
    "    \"\"\"\n",
    "    Op for element-wise application of ReLU function\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(context, input):\n",
    "        #print(input.shape)\n",
    "        relux = input * (input > 0)\n",
    "        context['relux'] = relux\n",
    "        return relux\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(context, goutput):\n",
    "        relux = context['relux']\n",
    "        drelux = np.greater(relux, 0).astype(int)\n",
    "        return drelux*goutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## loaded data:\n",
      "         number of instances: 60000 in training, 10000 in validation\n",
      " training class distribution: [32631 27369]\n",
      "     val. class distribution: [5541 4459]\n",
      "\n",
      "## Starting training\n",
      "epoch 000\n",
      "       accuracy: 0.5356\n",
      "   running loss: 5.238e+03\n",
      "epoch 001\n",
      "       accuracy: 0.9839\n",
      "   running loss: 2.528e+03\n",
      "epoch 002\n",
      "       accuracy: 0.9862\n",
      "   running loss: 2.286e+03\n",
      "epoch 003\n",
      "       accuracy: 0.9878\n",
      "   running loss: 2.18e+03\n",
      "epoch 004\n",
      "       accuracy: 0.9898\n",
      "   running loss: 2.123e+03\n",
      "epoch 005\n",
      "       accuracy: 0.9907\n",
      "   running loss: 2.094e+03\n",
      "epoch 006\n",
      "       accuracy: 0.9909\n",
      "   running loss: 2.069e+03\n",
      "epoch 007\n",
      "       accuracy: 0.9909\n",
      "   running loss: 2.045e+03\n",
      "epoch 008\n",
      "       accuracy: 0.9912\n",
      "   running loss: 2e+03\n",
      "epoch 009\n",
      "       accuracy: 0.9917\n",
      "   running loss: 1.994e+03\n",
      "epoch 010\n",
      "       accuracy: 0.9921\n",
      "   running loss: 1.988e+03\n",
      "epoch 011\n",
      "       accuracy: 0.9925\n",
      "   running loss: 1.986e+03\n",
      "epoch 012\n",
      "       accuracy: 0.9924\n",
      "   running loss: 1.977e+03\n",
      "epoch 013\n",
      "       accuracy: 0.9924\n",
      "   running loss: 1.964e+03\n",
      "epoch 014\n",
      "       accuracy: 0.9927\n",
      "   running loss: 1.921e+03\n",
      "epoch 015\n",
      "       accuracy: 0.9928\n",
      "   running loss: 1.902e+03\n",
      "epoch 016\n",
      "       accuracy: 0.9931\n",
      "   running loss: 1.887e+03\n",
      "epoch 017\n",
      "       accuracy: 0.9934\n",
      "   running loss: 1.87e+03\n",
      "epoch 018\n",
      "       accuracy: 0.9935\n",
      "   running loss: 1.859e+03\n",
      "epoch 019\n",
      "       accuracy: 0.9937\n",
      "   running loss: 1.836e+03\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import vugrad as vg\n",
    "\n",
    "# Parse command line arguments\n",
    "\n",
    "class args:\n",
    "    lr = 0.01\n",
    "    data = 'synth'\n",
    "    epochs = 20\n",
    "    batch_size = 128\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Wrap the sigmoid op in a funciton (just for symmetry with the softmax).\n",
    "\n",
    "    :param x:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return ReLU.do_forward(x)\n",
    "\n",
    "class MLP_ReLU(vg.MLP):\n",
    "    def forward(self, input):\n",
    "\n",
    "        assert len(input.size()) == 2\n",
    "\n",
    "        # first layer\n",
    "        hidden = self.layer1(input)\n",
    "\n",
    "        # non-linearity\n",
    "        hidden = relu(hidden)\n",
    "\n",
    "        # second layer\n",
    "        output = self.layer2(hidden)\n",
    "        output = vg.functions.softmax(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "## Load the data\n",
    "if args.data == 'synth':\n",
    "    (xtrain, ytrain), (xval, yval), num_classes = vg.load_synth()\n",
    "elif args.data == 'mnist':\n",
    "    (xtrain, ytrain), (xval, yval), num_classes = vg.load_mnist(final=False, flatten=True)\n",
    "else:\n",
    "    raise Exception(f'Dataset {args.data} not recognized.')\n",
    "\n",
    "print(f'## loaded data:')\n",
    "print(f'         number of instances: {xtrain.shape[0]} in training, {xval.shape[0]} in validation')\n",
    "print(f' training class distribution: {np.bincount(ytrain)}')\n",
    "print(f'     val. class distribution: {np.bincount(yval)}')\n",
    "\n",
    "num_instances, num_features = xtrain.shape\n",
    "\n",
    "from collections import defaultdict\n",
    "results = list()\n",
    "## Create the model.\n",
    "for nonlinearity, model in [('sigmoid', vg.MLP,), ('relu', MLP_ReLU)]\n",
    "    mlp = model(input_size=num_features, output_size=num_classes)\n",
    "\n",
    "    n, m = xtrain.shape\n",
    "    b = args.batch_size\n",
    "\n",
    "    print('\\n## Starting training')\n",
    "\n",
    "    cl = '...'\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "\n",
    "        print(f'epoch {epoch:03}')\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            ## Compute validation accuracy\n",
    "\n",
    "            o = mlp(vg.TensorNode(xval))\n",
    "            oval = o.value\n",
    "\n",
    "            predictions = np.argmax(oval, axis=1)\n",
    "            num_correct = (predictions == yval).sum()\n",
    "            acc = num_correct / yval.shape[0]\n",
    "\n",
    "            o.clear() # gc the computation graph\n",
    "\n",
    "            print(f'       accuracy: {acc:.4}')\n",
    "            \n",
    "        cl = 0.0 # running sum of the training loss\n",
    "\n",
    "        # We loop over the data in batches of size `b`\n",
    "        for fr in range(0, n, b):\n",
    "\n",
    "            # The end index of the batch\n",
    "            to = min(fr + b, n)\n",
    "\n",
    "            # Slice out the batch and its corresponding target values\n",
    "            batch, targets = xtrain[fr:to, :], ytrain[fr:to]\n",
    "\n",
    "            # Wrap the inputs in a Node\n",
    "            batch = vg.TensorNode(value=batch)\n",
    "\n",
    "            outputs = mlp(batch)\n",
    "            loss = vg.celoss(outputs, targets)\n",
    "            # -- The computation graph is now complete. It consists of the mlp, together with the computation of\n",
    "            #    the scalar loss.\n",
    "            # -- The variable `loss` is the TreeNode at the very top of our computation graph. This means we can call\n",
    "            #    it to perform operations on the computation graph, like clearing the gradients, starting the backpropgation\n",
    "            #    and clearing the graph.\n",
    "\n",
    "            cl += loss.value\n",
    "            # -- We must be careful here to extract the _raw_ value for the running loss. What would happen if we kept\n",
    "            #    a running sum using the TensorNode?\n",
    "\n",
    "            # Start the backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # pply gradient descent\n",
    "            for parm in mlp.parameters():\n",
    "                parm.value -= args.lr * parm.grad\n",
    "                # -- Note that we are directly manipulating the members of the parm TensorNode. This means that for this\n",
    "                #    part, we are not building up a computation graph.\n",
    "\n",
    "            # -- In Pytorch, the gradient descent is abstracted away into an Optimizer. This allows us to build slightly more\n",
    "            #    complexoptimizers than plain graident descent.\n",
    "\n",
    "            # Finally, we need to reset the gradients to zero ...\n",
    "            loss.zero_grad()\n",
    "            # ... and delete the parts of the computation graph we don't need to remember.\n",
    "            loss.clear()\n",
    "\n",
    "        print(f'   running loss: {cl:.4}')\n",
    "        results.append({\n",
    "                'epoch' : epoch,\n",
    "                'accuracy' : acc,\n",
    "                'loss' : cl,\n",
    "                'nonlinearity' : nonlinearity,\n",
    "                'dataset' : args.data\n",
    "            })\n",
    "\n",
    "import pandas as pd\n",
    "df = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4ee4b3bd58dba8b15a3a54ac18766444a5e3c5a9d8db984134e1e61525b723b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('aml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
