{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3SL8o-SEKey",
        "outputId": "22b712cc-b528-4f75-cf39-df31e1e0eb86"
      },
      "outputs": [],
      "source": [
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwrOrk74wRhn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "if torch.cuda.is_available():  \n",
        "  dev = \"cuda:0\" \n",
        "else:  \n",
        "  dev = \"cpu\"  \n",
        "device = torch.device(dev) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYzwe9kDE2a7"
      },
      "outputs": [],
      "source": [
        "import wget, os, gzip, pickle, random, re, sys\n",
        "\n",
        "IMDB_URL = 'http://dlvu.github.io/data/imdb.{}.pkl.gz'\n",
        "IMDB_FILE = 'imdb.{}.pkl.gz'\n",
        "\n",
        "PAD, START, END, UNK = '.pad', '.start', '.end', '.unk'\n",
        "\n",
        "def load_imdb(final=False, val=5000, seed=0, voc=None, char=False):\n",
        "\n",
        "    cst = 'char' if char else 'word'\n",
        "\n",
        "    imdb_url = IMDB_URL.format(cst)\n",
        "    imdb_file = IMDB_FILE.format(cst)\n",
        "\n",
        "    if not os.path.exists(imdb_file):\n",
        "        wget.download(imdb_url)\n",
        "\n",
        "    with gzip.open(imdb_file) as file: \n",
        "        sequences, labels, i2w, w2i = pickle.load(file)\n",
        "\n",
        "    if voc is not None and voc < len(i2w):\n",
        "        nw_sequences = {}\n",
        "\n",
        "        i2w = i2w[:voc]\n",
        "        w2i = {w: i for i, w in enumerate(i2w)}\n",
        "\n",
        "        mx, unk = voc, w2i['.unk']\n",
        "        for key, seqs in sequences.items():\n",
        "            nw_sequences[key] = []\n",
        "            for seq in seqs:\n",
        "                seq = [s if s < mx else unk for s in seq]\n",
        "                nw_sequences[key].append(seq)\n",
        "\n",
        "        sequences = nw_sequences\n",
        "\n",
        "    if final:\n",
        "        return (sequences['train'], labels['train']), (sequences['test'], labels['test']), (i2w, w2i), 2\n",
        "\n",
        "    # Make a validation split\n",
        "    random.seed(seed)\n",
        "\n",
        "    x_train, y_train = [], []\n",
        "    x_val, y_val = [], []\n",
        "\n",
        "    val_ind = set( random.sample(range(len(sequences['train'])), k=val) )\n",
        "    for i, (s, l) in enumerate(zip(sequences['train'], labels['train'])):\n",
        "        if i in val_ind:\n",
        "            x_val.append(s)\n",
        "            y_val.append(l)\n",
        "        else:\n",
        "            x_train.append(s)\n",
        "            y_train.append(l)\n",
        "\n",
        "    return (x_train, y_train), \\\n",
        "           (x_val, y_val), \\\n",
        "           (i2w, w2i), 2\n",
        "\n",
        "\n",
        "def gen_sentence(sent, g):\n",
        "\n",
        "    symb = '_[a-z]*'\n",
        "\n",
        "    while True:\n",
        "\n",
        "        match = re.search(symb, sent)\n",
        "        if match is None:\n",
        "            return sent\n",
        "\n",
        "        s = match.span()\n",
        "        sent = sent[:s[0]] + random.choice(g[sent[s[0]:s[1]]]) + sent[s[1]:]\n",
        "\n",
        "def gen_dyck(p):\n",
        "    open = 1\n",
        "    sent = '('\n",
        "    while open > 0:\n",
        "        if random.random() < p:\n",
        "            sent += '('\n",
        "            open += 1\n",
        "        else:\n",
        "            sent += ')'\n",
        "            open -= 1\n",
        "\n",
        "    return sent\n",
        "\n",
        "def gen_ndfa(p):\n",
        "\n",
        "    word = random.choice(['abc!', 'uvw!', 'klm!'])\n",
        "\n",
        "    s = ''\n",
        "    while True:\n",
        "        if random.random() < p:\n",
        "            return 's' + s + 's'\n",
        "        else:\n",
        "            s+= word\n",
        "\n",
        "def load_brackets(n=50_000, seed=0):\n",
        "    return load_toy(n, char=True, seed=seed, name='dyck')\n",
        "\n",
        "def load_ndfa(n=50_000, seed=0):\n",
        "    return load_toy(n, char=True, seed=seed, name='ndfa')\n",
        "\n",
        "def load_toy(n=50_000, char=True, seed=0, name='lang'):\n",
        "\n",
        "    random.seed(0)\n",
        "\n",
        "    if name == 'lang':\n",
        "        sent = '_s'\n",
        "\n",
        "        toy = {\n",
        "            '_s': ['_s _adv', '_np _vp', '_np _vp _prep _np', '_np _vp ( _prep _np )', '_np _vp _con _s' , '_np _vp ( _con _s )'],\n",
        "            '_adv': ['briefly', 'quickly', 'impatiently'],\n",
        "            '_np': ['a _noun', 'the _noun', 'a _adj _noun', 'the _adj _noun'],\n",
        "            '_prep': ['on', 'with', 'to'],\n",
        "            '_con' : ['while', 'but'],\n",
        "            '_noun': ['mouse', 'bunny', 'cat', 'dog', 'man', 'woman', 'person'],\n",
        "            '_vp': ['walked', 'walks', 'ran', 'runs', 'goes', 'went'],\n",
        "            '_adj': ['short', 'quick', 'busy', 'nice', 'gorgeous']\n",
        "        }\n",
        "\n",
        "        sentences = [ gen_sentence(sent, toy) for _ in range(n)]\n",
        "        sentences.sort(key=lambda s : len(s))\n",
        "\n",
        "    elif name == 'dyck':\n",
        "\n",
        "        sentences = [gen_dyck(7./16.) for _ in range(n)]\n",
        "        sentences.sort(key=lambda s: len(s))\n",
        "\n",
        "    elif name == 'ndfa':\n",
        "\n",
        "        sentences = [gen_ndfa(1./4.) for _ in range(n)]\n",
        "        sentences.sort(key=lambda s: len(s))\n",
        "\n",
        "    else:\n",
        "        raise Exception(name)\n",
        "\n",
        "    tokens = set()\n",
        "    for s in sentences:\n",
        "\n",
        "        if char:\n",
        "            for c in s:\n",
        "                tokens.add(c)\n",
        "        else:\n",
        "            for w in s.split():\n",
        "                tokens.add(w)\n",
        "\n",
        "    i2t = [PAD, START, END, UNK] + list(tokens)\n",
        "    t2i = {t:i for i, t in enumerate(i2t)}\n",
        "\n",
        "    sequences = []\n",
        "    for s in sentences:\n",
        "        if char:\n",
        "            tok = list(s)\n",
        "        else:\n",
        "            tok = s.split()\n",
        "        sequences.append([t2i[t] for t in tok])\n",
        "\n",
        "    return sequences, (i2t, t2i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dR08t5ZTE6Og"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Load data \n",
        "(x_train, y_train), (x_val, y_val), (i2w, w2i), numcls = load_imdb(final=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21q_33ffxaYo",
        "outputId": "1484bd55-2e50-453f-fde9-21c29a163f5c"
      },
      "outputs": [],
      "source": [
        "print([i2w[w] for w in x_train[141]])\n",
        "print(len(x_train))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQs3EFPEye7u",
        "outputId": "d8de44e2-b8dc-4562-df2b-34df78e076a5"
      },
      "outputs": [],
      "source": [
        "print(w2i['.pad'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qpzrNX_HL-H",
        "outputId": "d7f688d9-b90d-4479-8b2a-baaccc443c96"
      },
      "outputs": [],
      "source": [
        "# Pad x data to right size\n",
        "\n",
        "# w2i['.pad'] = 0\n",
        "pad_val = 0\n",
        "# w2i['.start'] = 1\n",
        "start_val = 1\n",
        "# w2i['.end'] = 2\n",
        "end_val = 2\n",
        "\n",
        "longest_len = max([len(x) for x in x_train]) +2 # +2 because appending start and end val\n",
        "print(longest_len)\n",
        "for review in x_train:\n",
        "  review.insert(0, start_val)\n",
        "  review.append(end_val)\n",
        "  while len(review)< longest_len:\n",
        "    review.append(pad_val)\n",
        "\n",
        "\n",
        "# Verify\n",
        "print(x_train[0])\n",
        "print([len(x) for x in x_train])\n",
        "print(max([len(x) for x in x_train]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfdChSKQJl01",
        "outputId": "4102347f-b2a4-4d17-a600-c603755578c2"
      },
      "outputs": [],
      "source": [
        "# Same procedure for x_val:\n",
        "longest_len = max([len(x) for x in x_val]) +2 # +2 because appending start and end val\n",
        "print(longest_len)\n",
        "for review in x_val:\n",
        "  review.insert(0, start_val)\n",
        "  review.append(end_val)\n",
        "  while len(review)< longest_len:\n",
        "    review.append(pad_val)\n",
        "\n",
        "\n",
        "# Verify\n",
        "#print(x_val[0])\n",
        "#print([len(x) for x in x_val])\n",
        "print(max([len(x) for x in x_val]))\n",
        "len(i2w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hi7CP519HKjX"
      },
      "outputs": [],
      "source": [
        "x_train = torch.Tensor(x_train).type(torch.LongTensor)\n",
        "y_train = torch.Tensor(y_train).type(torch.LongTensor)\n",
        "\n",
        "x_val = torch.Tensor(x_val).type(torch.LongTensor)\n",
        "y_val = torch.Tensor(y_val).type(torch.LongTensor)\n",
        "\n",
        "#batch = torch.tensor(lists, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMhRXJqbFmTq"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader([[x_train[i], y_train[i]] for i in range(len(y_train))], batch_size, shuffle=True)\n",
        "valloader = torch.utils.data.DataLoader([[x_val[i], y_val[i]] for i in range(len(y_val))], batch_size, shuffle= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrTFfzgrCWXH"
      },
      "outputs": [],
      "source": [
        "def train(epochs): \n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        \n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "         \n",
        "            # Extract input and label correctly\n",
        "            inputs, labels = data\n",
        "            \n",
        "            #zero param grads\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            #forward\n",
        "            outputs = net(inputs)\n",
        "            print(outputs)\n",
        "            softmaxed = F.Softmax(outputs)\n",
        "            #print(labels.shape)\n",
        "            #loss\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "            \n",
        "            #backward\n",
        "            loss.backward()\n",
        "            #optimize\n",
        "            optimizer.step()\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            if i % 1000 == 999: #print every 1000 batches\n",
        "                print('[%d, %5d] loss: %.3f ' %\n",
        "                      (epoch +1, i+1, running_loss / 1000))\n",
        "                running_loss = 0.0\n",
        "    print('Finished training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4tNrnbxUoIE"
      },
      "outputs": [],
      "source": [
        "def test(model):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(valloader):\n",
        "          if i == 100:\n",
        "            break\n",
        "          images, labels = data\n",
        "            \n",
        "            #calc output by running images through network\n",
        "          outputs = model(images)\n",
        "          _, predicted = torch.max(outputs.data,1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "            \n",
        "    print('Accuracy of network is ', correct / total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrAAgPqVqspg"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2N6FD6qNiAc"
      },
      "source": [
        "# PART III"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8ndWHisOUaS"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class Elman(nn.Module):\n",
        "  \n",
        "  def __init__(self, insize = 300, outsize = 300, hsize = 300):\n",
        "    super().__init__()\n",
        "\n",
        "    self.lin1 =  nn.Linear(in_features= insize + hsize, out_features=hsize)\n",
        "    self.lin2 =  nn.Linear(in_features= hsize, out_features= outsize)\n",
        "  \n",
        "  def forward(self, x, hidden = None):\n",
        "    b, t, e = x.size()\n",
        "\n",
        "    if hidden is None:\n",
        "      hidden = torch.zeros(b, e, dtype=torch.float).to(device)\n",
        "\n",
        "    outs = []\n",
        "    for i in range(t):\n",
        "      inp = torch.cat([x[:, i, :], hidden], dim = 1)\n",
        "      hidden = self.lin1(inp)\n",
        "      hidden = F.relu(hidden)\n",
        "      yi = self.lin2(hidden)\n",
        "      out = yi\n",
        "    \n",
        "      outs.append(out[:, None, :])\n",
        "\n",
        "    return torch.cat(outs, dim=1), hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxTWIwbjQn3U",
        "outputId": "b4efe5b0-ce3c-4a55-e307-d33b690a42b1"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# input shape x_train = 4 x 2520\n",
        "class ElNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, output_size):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        #Input nn.Embedding: num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None, device=None, dtype=None\n",
        "        # len(i2w) = 99430\n",
        "        embedding_dim = 300\n",
        "        hidden_size = embedding_dim  #idk what to pick here?\n",
        "        num_cls = 2 #labrl of y 0,1 positive/negative review\n",
        "\n",
        "        self.layer1 = nn.Embedding(num_embeddings = 99430, embedding_dim = embedding_dim , padding_idx = 0) #num embeddings is tot num of tokens?? 99430\n",
        "       \n",
        "        \n",
        "        self.layer2 = Elman(insize = embedding_dim, outsize= hidden_size, hsize = 300)\n",
        "\n",
        "        self.layer3 = nn.Linear(embedding_dim, num_cls)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        \n",
        "        \n",
        "        #print(input.shape)\n",
        "        #first layer\n",
        "        emb = self.layer1(input)\n",
        "         #Does embedding with padding_idx cut to same length within batch?\n",
        "        \n",
        "        #second layer   \n",
        "        # should only work on last dimension, thus 2,emb\n",
        "        el_out, hidden = self.layer2(emb)\n",
        "        \n",
        "        #non-linearity\n",
        "        non_linear_hidden = F.relu(el_out)\n",
        "\n",
        "        permuted = non_linear_hidden.permute(0,2,1)\n",
        "        \n",
        "        #maxpool\n",
        "        pooled = F.max_pool1d(permuted, kernel_size=permuted.shape[2]).permute(0,2,1)\n",
        "        \n",
        "        pooled = torch.squeeze(pooled, 1)\n",
        "        output = self.layer3(pooled)\n",
        "        #print(pooled.shape)\n",
        "        \n",
        "        \n",
        "        return output\n",
        "\n",
        "elman = ElNet(x_train.shape[1], 2)\n",
        "print(elman)\n",
        "elman.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2CcbdoXV8-M"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def train(epochs, model, GPU):\n",
        "  clip = 5\n",
        "  import torch.optim as optim\n",
        "  criterion = nn.BCELoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
        " \n",
        "  data = list()\n",
        "  for epoch in range(epochs):\n",
        "  \n",
        "        print(\"Epoch \", epoch)\n",
        "        loss_epoch = 0\n",
        "        running_loss = 0.0\n",
        "        losses = []\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            #if i < 500:\n",
        "              # Extract input and label correctly\n",
        "              inputs, labels = data\n",
        "              if GPU:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "              \n",
        "              #zero param grads\n",
        "              optimizer.zero_grad()\n",
        "              \n",
        "              #forward\n",
        "              outputs = model(inputs)\n",
        "              outputs = F.softmax(outputs)\n",
        "              \n",
        "              #loss\n",
        "              labels = F.one_hot(labels).float()\n",
        "              \n",
        "              loss = criterion(outputs, labels)\n",
        "              \n",
        "              #backward\n",
        "              loss.backward()\n",
        "\n",
        "              #gradient clipping\n",
        "              nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "              #optimize\n",
        "              optimizer.step()\n",
        "              \n",
        "              running_loss += loss.item()\n",
        "              loss_epoch += loss.item()\n",
        "              losses.append(loss.item()) \n",
        "              if i % 10 == 9: #print every 1000 batches\n",
        "                  print('[%d, %5d] loss: %.3f ' %\n",
        "                        (epoch +1, i+1, running_loss / 9))\n",
        "                  running_loss = 0.0\n",
        "              data.append({'update' : i, 'epoch': epoch, 'loss': loss.item()})\n",
        "  print('Finished training')\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "Fjg_RMqsRrPz",
        "outputId": "12e1f243-abdd-44c0-cad5-6daef1b46b71"
      },
      "outputs": [],
      "source": [
        "data_implemented = train(2, elman, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQQxi9BFchtv",
        "outputId": "cc37e510-7be2-42a7-b49b-0e39b839ae14"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# input shape x_train = 4 x 2520\n",
        "class ElNNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, output_size):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        #Input nn.Embedding: num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None, device=None, dtype=None\n",
        "        # len(i2w) = 99430\n",
        "        embedding_dim = 300\n",
        "        hidden_size = embedding_dim  #idk what to pick here?\n",
        "        num_cls = 2 #labrl of y 0,1 positive/negative review\n",
        "\n",
        "        self.layer1 = nn.Embedding(num_embeddings = 99430, embedding_dim = embedding_dim , padding_idx = 0) #num embeddings is tot num of tokens?? 99430\n",
        "       \n",
        "        \n",
        "        self.layer2 = nn.RNN(input_size =embedding_dim,hidden_size= hidden_size, num_layers=1, batch_first=True)\n",
        "\n",
        "        \n",
        "        self.layer3 = nn.Linear(embedding_dim, num_cls)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        \n",
        "        \n",
        "        #print(input.shape)\n",
        "        #first layer\n",
        "        emb = self.layer1(input)\n",
        "         #Does embedding with padding_idx cut to same length within batch?\n",
        "       \n",
        "        #second layer   \n",
        "        # should only work on last dimension, thus 2,emb\n",
        "        el_out, hidden = self.layer2(emb)\n",
        "        \n",
        "        #non-linearity\n",
        "        non_linear_hidden = F.relu(el_out)\n",
        "\n",
        "        permuted = non_linear_hidden.permute(0,2,1)\n",
        "        #print(permuted.shape)\n",
        "        #maxpool\n",
        "        pooled = F.max_pool1d(permuted, kernel_size=permuted.shape[2]).permute(0,2,1)\n",
        "        #print(pooled.shape)\n",
        "        pooled = torch.squeeze(pooled, 1)\n",
        "        output = self.layer3(pooled)\n",
        "        #print(pooled.shape)\n",
        "        \n",
        "        \n",
        "        return output\n",
        "elmaNN = ElNNet(x_train.shape[1], 2)\n",
        "print(elmaNN)\n",
        "elmaNN.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gMvesAWEVxMa",
        "outputId": "d94a571a-5f8f-49f7-b75e-5b09ab01f830"
      },
      "outputs": [],
      "source": [
        "lossesElman = train(1, elman, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhzM46CO4B3W",
        "outputId": "a6f77326-1f6a-4894-f611-b64b7330ea43"
      },
      "outputs": [],
      "source": [
        "lossesElmaNN = train(1, elmaNN, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1gc9G4reisY"
      },
      "outputs": [],
      "source": [
        "test(elmann)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "zApJ2N-ub2pT",
        "outputId": "75d6cf6a-7bf4-466b-afc5-1439de493e97"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DL3: RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
